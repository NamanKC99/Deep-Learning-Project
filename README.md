â€‹ğŸ“‹ Project Description
â€‹Human Activity Recognition (HAR) is a key area in mobile health and context-aware computing. This project leverages Deep Learning (typically using Convolutional Neural Networks (CNNs) or Long Short-Term Memory (LSTM) networks) to identify specific human movements based on sensor data.
â€‹The system processes data from smartphone sensorsâ€”specifically the accelerometer and gyroscopeâ€”to classify physical activities in real-time or from recorded datasets.
â€‹ğŸš€ Key Features
â€‹Time-Series Analysis: Processes sequential data from 3-axial linear acceleration and 3-axial angular velocity.
â€‹Multi-Class Classification: Successfully recognizes various activities such as:
â€‹ğŸš¶ Walking
â€‹ğŸªœ Walking Upstairs / Downstairs
â€‹ğŸª‘ Sitting
â€‹ğŸš¶â€â™‚ï¸ Standing
â€‹ğŸ›Œ Laying
â€‹End-to-End Pipeline: Covers data preprocessing (noise filtering, sliding window splitting), feature extraction, and model evaluation using confusion matrices and accuracy scores.
â€‹ğŸ› ï¸ Tech Stack
â€‹Language: Python
â€‹Deep Learning Frameworks: TensorFlow / Keras or PyTorch
â€‹Data Libraries: NumPy, Pandas, Matplotlib, Scikit-learn
â€‹Dataset: (Specify your dataset, e.g., UCI HAR Dataset or WISDM)
â€‹ğŸ§  Model Architecture
â€‹The project explores advanced neural network architectures designed for spatial and temporal feature extraction:
â€‹CNN (1D): Extracts local patterns across sensor signal windows.
â€‹LSTM / GRU: Captures temporal dependencies and the "flow" of movement over time.
â€‹Hybrid CNN-LSTM: Combines spatial feature extraction with temporal sequence modeling for high-precision results.
